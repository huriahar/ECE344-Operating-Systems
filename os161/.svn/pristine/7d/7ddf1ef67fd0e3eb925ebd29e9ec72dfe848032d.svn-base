#include <types.h>
#include <kern/errno.h>
#include <lib.h>
#include <thread.h>
#include <curthread.h>
#include <addrspace.h>
#include <vm.h>
#include <machine/spl.h>
#include <machine/tlb.h>
#include <synch.h>
#include <elf.h>
#include <kern/unistd.h>
#include <vfs.h>
#include <vnode.h>
#include <kern/stat.h>
#include <uio.h>
#include <clock.h>

int vm_bootstrap_done = 0;

struct cmap_entry *cmap;
struct lock* access_cmap;
unsigned long page_count;

struct vnode *swap_file;
struct smap_entry *smap;
unsigned long smap_page_count;

// --------------------- in RAM---------------------------
// [smap_start_physaddr, smap_start_physaddr + smap_size] --> smap
// [cmap_start_physaddr, cmap_start_physaddr + cmap_size) --> coremap
// [cmap_start_physaddr + cmap_size,  last_physaddr] --> system's free memory

// -----------In the coremap-----------------------------
// [0			 , cmapsize) --> fixed  - cmap
// [cmapsize, last_physaddr] --> freed

void vm_bootstrap(void)
{
//	unsigned long i;
	
	//--------------------------------------- smap ----------------------------------------------------
	unsigned long smap_size;
	paddr_t smap_start_physaddr;
	unsigned long i;

	char file_name[9];
	strcpy(file_name,"lhd0raw:");
	
	int ret = vfs_open(file_name, O_RDWR, &swap_file);
	assert(ret == 0);

	struct stat swap_status;
	VOP_STAT(swap_file, &swap_status);

	smap_page_count = swap_status.st_size / PAGE_SIZE;
	smap_size = smap_page_count * sizeof(struct smap_entry);
	smap_size = DIVROUNDUP(smap_size, PAGE_SIZE);
	smap_start_physaddr = ram_stealmem(smap_size);
	smap = (struct smap_entry *) PADDR_TO_KVADDR(smap_start_physaddr);

	for(i = 0; i < smap_page_count; i++){
		smap[i].disk_pa = (i*PAGE_SIZE);
		smap[i].state = empty;
		smap[i].as = NULL;
		smap[i].va = 0;
	}

	smap_pages_avail = smap_page_count;

	//--------------------------------------- coremap ----------------------------------------------------
	paddr_t first_physaddr, last_physaddr;
	paddr_t cmap_start_physaddr;
	unsigned long cmap_size;

	ram_getsize(&first_physaddr, &last_physaddr);	

	page_count = (last_physaddr - first_physaddr) / PAGE_SIZE; 			//number of pages the physical memory can store
	cmap_size = page_count * sizeof(struct cmap_entry);					//need to allocate page_count entries in the cmap in order to keep track of each entry
	cmap_size = DIVROUNDUP(cmap_size, PAGE_SIZE);						//round the cmap size to the nearest page
	cmap_start_physaddr = ram_stealmem(cmap_size);						//pass number of pages we want to "steal". stolen pages won't be freed
	cmap = (struct cmap_entry*) PADDR_TO_KVADDR(cmap_start_physaddr);	//allocate the cmap in the first part of the physical memory

	for(i = 0; i < page_count; i++){
		if(i < cmap_size){
			cmap[i].state = fixed;
		}
		else{
			cmap[i].state = freed;
			cmap[i].num_pages = -1;
			cmap[i].first_page = 0;
		}
		cmap[i].pa = cmap_start_physaddr + (i*PAGE_SIZE);
		cmap[i].as = NULL;
		cmap[i].s = 0;
		cmap[i].ns = 0;
	}

	pages_avail = page_count - cmap_size;

	ram_reset();

	vm_bootstrap_done = 1;
}

void free_all_pages(struct addrspace *as){
	unsigned long i;
	for(i = 0; i < page_count; i++){
		if(cmap[i].as == as)
			free_kpages(PADDR_TO_KVADDR(cmap[i].pa));
	}
}

/*
	return a pointer to the beginning of the physical address space allocated
	allocating continous pages
*/

paddr_t getppages(unsigned long npages, page_state_t pstate)
{
	paddr_t ret_addr = 0;	//alloc_upages excepts a 0 upon failure
	if(vm_bootstrap_done == 0){
		//if need to kmalloc before coremap is setup, then we're calling ram_stealmem
		int spl = splhigh();
		ret_addr = ram_stealmem(npages);
		splx(spl);
	}
	else{
		//just allocate npages from the pages marked as free in the coremap
		//lock_acquire(access_cmap);
		int spl = splhigh();
		//kprintf("pages avail = %d\n", pages_avail);
	//	assert(npages <= pages_avail);
		//search the coremap for npages free pages
		if(npages <= pages_avail){
			unsigned long i, j, k, nfound = 0;
			for(i = 0; i < page_count; i++){
				if (cmap[i].state == freed) {
					nfound++;

					if (nfound == npages){
						for(j = i, k=0; j > i-npages; j--, k++){
							cmap[j].as = curthread->t_vmspace;
							cmap[j].state = dirty;
							cmap[j].num_pages = npages;
							cmap[j].p_state = pstate;
							time_t s;
							u_int32_t ns;
							
							gettime(&s, &ns);
							cmap[j].s = s;
							cmap[j].ns = ns;
							
							if(k==npages-1){ //first page in block
								cmap[j].first_page = 1;
								ret_addr = cmap[j].pa;
								pages_avail -= npages;
							}
						}
						break;
					}
				}
				else{
					nfound = 0;
				}

			}
		}
		splx(spl);
	}
	return ret_addr;
}

/* Allocate/free some kernel-space virtual pages */
vaddr_t 
alloc_kpages(int npages)
{
	paddr_t pa;
	pa = getppages(npages, kernel);
	if (pa==0) {
		return 0;
	}
	return PADDR_TO_KVADDR(pa);
}

vaddr_t 
alloc_upages(int npages)
{
	paddr_t pa;
	pa = getppages(npages, user);
	if (pa==0) {
		return 0;
	}
	return PADDR_TO_KVADDR(pa);
}

void 
free_kpages(vaddr_t addr)
{
	int spl = splhigh();
	//lock_acquire(access_cmap);
	paddr_t phy_addr = KVADDR_TO_PADDR(addr);
	unsigned long i, j;
	for(i = 0; i < page_count; i++) {
		if (cmap[i].pa == phy_addr)
			break;
	}

	if (cmap[i].first_page == 1){
		unsigned long length = cmap[i].num_pages;
		for (j = 0; j < length ; j++){
			cmap[i+j].state = freed;
			cmap[i+j].num_pages = -1;
		}
		cmap[i].first_page = 0;
		pages_avail += length;
	}
	splx(spl);
}

unsigned long find_lru(){
	unsigned long i;
	unsigned long ret = page_count;

	time_t min_s;
	u_int32_t min_ns;
	
	gettime(&min_s, &min_ns);

	for(i = 0; i < page_count; i++){
		assert(cmap[i].state != freed);	//there cannot be any freed pages at this point. if there are freed pages, a page would have been allocated and no need to swap out
		if(cmap[i].state == fixed) continue;
		if(cmap[i].p_state == kernel) continue;

		if(cmap[i].s < min_s){
			min_s = cmap[i].s;
			min_ns = cmap[i].ns;
			ret = i;
		}
		else if(cmap[i].s == min_s){
			if(cmap[i].ns < min_ns){
				min_s = cmap[i].s;
				min_ns = cmap[i].ns;
				ret = i;
			}
		}
	}
	assert(ret < page_count);
	return ret;
}

unsigned long get_space_on_disk(unsigned long index, vaddr_t faultaddress){
	struct addrspace *as = cmap[index].as;
	unsigned long i;
	for(i = 0; i < smap_page_count; i++){
		if(smap[i].as == as && smap[i].va == faultaddress)
			break;
		if(smap[i].state == empty){
			assert( i >= (smap_page_count - smap_pages_avail));
			break;
		}
	}
	assert(i < smap_page_count);

	smap[i].state = occupied;
	smap[i].as = as;
	smap[i].va = faultaddress;

	return i;
}

/*
	writes the old page content to disk, updates the smap
*/
void swap_out(unsigned long offset,vaddr_t va){
	struct uio uio_swap;
	
	mk_kuio(&uio_swap,(void *)(va & PAGE_FRAME), PAGE_SIZE, offset, UIO_WRITE);

	int ret = VOP_WRITE(swap_file, &uio_swap);
	if(ret){
		panic("couldn't write to disk");
	}
	//invalidate the evicted tlb entry
	u_int32_t ehi,elo,i;
	paddr_t pa = KVADDR_TO_PADDR(va);

	for (i = 0; i < NUM_TLB; i++) {

		TLB_Read(&ehi, &elo, i);

		if ((elo & PAGE_FRAME) == (pa & PAGE_FRAME))	{
			TLB_Write(TLBHI_INVALID(i), TLBLO_INVALID(), i);		
		}
	}
	return;
}

void update_pte(unsigned long index){
	struct pte *old_entry = cmap[index].as->pages;
	while(old_entry != cmap[index].as->heap){
		if(old_entry->pa == cmap[index].pa)
			break;
	}
	assert(old_entry != cmap[index].as->heap);
	old_entry->on_mem = 0;
}

paddr_t make_space(vaddr_t faultaddress){
	unsigned long lru_index = find_lru();
	unsigned long offset = get_space_on_disk(lru_index, faultaddress);
	swap_out( offset, PADDR_TO_KVADDR(cmap[lru_index].pa));
	update_pte(lru_index);
}

paddr_t demand_page(struct pte *entry){
	assert (entry != NULL);
	int spl = splhigh();
	paddr_t pa = 0;

	if(pages_avail > 0){
		pa = KVADDR_TO_PADDR(alloc_upages(1));	//allocate one page
		if(pa == 0){
			splx(spl);
			return ENOMEM;	//oops ran out of memory!!! :'(
		}
	}
	else{
		//TODO: entry's va is the same as the faultaddress???
		//TODO: we're not freeing all the pages
		pa = make_space(entry->va);
	}
	entry->pa = pa;
	entry->on_mem = 1;
	assert((pa & PAGE_FRAME) == pa);
	splx(spl);
	return entry->pa;
}


int vm_fault(int faulttype, vaddr_t faultaddress)
{
	
	struct addrspace *as;
	int spl;

	spl = splhigh();

	faultaddress &= PAGE_FRAME;

	DEBUG(DB_VM, "dumbvm: fault: 0x%x\n", faultaddress);

	switch (faulttype) {
	    case VM_FAULT_READONLY:
		//We always create pages read-write, so we can't get this 
		panic("dumbvm: got VM_FAULT_READONLY\n");
	    case VM_FAULT_READ:
	    case VM_FAULT_WRITE:
		break;
	    default:
		splx(spl);
		return EINVAL;
	}

	as = curthread->t_vmspace;
	if (as == NULL) {
		
		// No address space set up. This is probably a kernel
		// fault early in boot. Return EFAULT so as to panic
		// instead of getting into an infinite faulting loop.
		
		splx(spl);
		return EFAULT;
	}

	// Assert that the address space has been set up properly. 

	assert(as->pages != NULL);
	assert(as->last_page != NULL);
	assert(as->stack != NULL);
	assert(as->last_stack != NULL);
	assert(as->heap != NULL);
	assert(as->heap_begin != 0);
	assert(as->heap_end != 0);
	assert(as->regions != NULL);
	assert(as->last_region != NULL);
	assert((as->pages->va & PAGE_FRAME) == as->pages->va);
	assert((as->pages->pa & PAGE_FRAME) == as->pages->pa);
	assert((as->regions->pa & PAGE_FRAME) == as->regions->pa);
	assert((as->regions->pa & PAGE_FRAME) == as->regions->pa);
	assert(faultaddress <= MIPS_KSEG0);

	struct pte* entry;
	paddr_t pa;

	if(faultaddress >= as->stack_begin && faultaddress < as->stack_end){
		entry = as->stack;
		while(entry != NULL){	//search for the address in each of the stack pages
			if(faultaddress >= entry->va && faultaddress < (entry->va + PAGE_SIZE)){
				pa = entry->pa;
				break;
			}
			entry = entry->next;
		}
	}
	else if(faultaddress >= as->heap_begin && faultaddress < as->heap_end){
		entry = as->heap;
		while(entry != NULL){	//search for the address in each of the stack pages
			if(faultaddress >= entry->va && faultaddress < (entry->va + PAGE_SIZE)){
				pa = entry->pa;
				break;
			}
			entry = entry->next;
		}
	}
	else if(faultaddress >= as->pages->va && faultaddress < (as->last_page->va + PAGE_SIZE)){
		entry = as->pages;
		while(entry != NULL){	//search for the address in each of the stack pages
			if(faultaddress >= entry->va && faultaddress < (entry->va + PAGE_SIZE)){
				pa = entry->pa;
				break;
			}
			entry = entry->next;   
		}
	}
	else {
		splx(spl);
		return EFAULT;
	}

	assert(entry->va == faultaddress);

	if(pa == 0 || pa == 0xdeadbeef){ //not in memory, but no page is allocated, need to allocate a new page
		assert(entry->on_mem == 0);
		pa = demand_page(entry);
	}

	// make sure it's page-aligned 
	assert((pa & PAGE_FRAME)==pa);

	u_int32_t ehi, elo;
	int i;
	for (i = 0; i < NUM_TLB; i++) {
		TLB_Read(&ehi, &elo, i);
		if (elo & TLBLO_VALID) {
			continue;
		}
		ehi = faultaddress;
		elo = pa | TLBLO_DIRTY | TLBLO_VALID;
		DEBUG(DB_VM, "dumbvm: 0x%x -> 0x%x\n", faultaddress, pa);
		TLB_Write(ehi, elo, i);
		splx(spl);
		return 0;
	}

	ehi = faultaddress;
	elo = pa | TLBLO_DIRTY | TLBLO_VALID;
	TLB_Random(ehi, elo);
	splx(spl);
	return 0;
}
